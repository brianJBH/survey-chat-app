import streamlit as st
import pandas as pd
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
import os

# ğŸ”‘ OpenAI API í‚¤ ì„¤ì •
openai_api_key = st.sidebar.text_input("Enter your OpenAI API key", type="password")
if not openai_api_key:
    st.warning("Please enter your OpenAI API key.")
    st.stop()

os.environ["OPENAI_API_KEY"] = openai_api_key

st.title("ğŸ“Š ì„¤ë¬¸ ì‘ë‹µ ì±—ë´‡")
uploaded_file = st.file_uploader("CSV ë˜ëŠ” Excel íŒŒì¼ ì—…ë¡œë“œ", type=["csv", "xlsx"])

if uploaded_file:
    # ğŸ”„ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    if uploaded_file.name.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.read_excel(uploaded_file)

    st.write("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", df.head())

    # ğŸ§© ë°ì´í„° ì „ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ë³€í™˜
    docs = []
    for i, row in df.iterrows():
        row_text = "\n".join([f"{col}: {val}" for col, val in row.items() if pd.notna(val)])
        docs.append(row_text)

    # ğŸ“ ë¬¸ì„œ ë‚˜ëˆ„ê¸° (ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ)
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.create_documents(docs)

    # ğŸ§  ë²¡í„° DB ë§Œë“¤ê¸° (FAISS)
    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(texts, embeddings)

    # ğŸ§¾ ì§ˆì˜ì‘ë‹µ ì²´ì¸ ë§Œë“¤ê¸°
    qa = RetrievalQA.from_chain_type(
        llm=OpenAI(temperature=0),
        retriever=db.as_retriever(),
        return_source_documents=True
    )

    # ğŸ’¬ ì§ˆë¬¸ ì…ë ¥
    query = st.text_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš” (ì˜ˆ: 'ê²½ì˜ì§„ ë§Œì¡±ë„ëŠ” ì–´ë• ë‚˜ìš”?')")

    if query:
        result = qa({"query": query})
        st.subheader("ğŸ’¡ ë‹µë³€")
        st.write(result["result"])

        with st.expander("ğŸ” ê´€ë ¨ ë°ì´í„° ë³´ê¸°"):
            for doc in result["source_documents"]:
                st.markdown(f"- {doc.page_content}")

